{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffadb9da",
   "metadata": {},
   "source": [
    "# A/B Testing Analysis: Prompt Optimization\n",
    "\n",
    "Statistical analysis and visualization of prompt performance comparison.\n",
    "\n",
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c83a0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "# Load results\n",
    "results_path = os.path.join('..', 'results', 'results.csv')\n",
    "df = pd.read_csv(results_path)\n",
    "\n",
    "print(f\"ðŸ“Š Loaded {len(df)} evaluation results\")\n",
    "print(f\"Unique queries: {df['query'].nunique()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec50f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance summary\n",
    "summary = df.groupby('prompt_version').agg({\n",
    "    'score': ['mean', 'std'],\n",
    "    'latency_ms': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "print(\"ðŸ“ˆ Performance Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf19b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical significance testing\n",
    "a_scores = df[df['prompt_version'] == 'A']['score']\n",
    "b_scores = df[df['prompt_version'] == 'B']['score']\n",
    "\n",
    "a_latency = df[df['prompt_version'] == 'A']['latency_ms']\n",
    "b_latency = df[df['prompt_version'] == 'B']['latency_ms']\n",
    "\n",
    "score_test = stats.ttest_ind(a_scores, b_scores)\n",
    "latency_test = stats.ttest_ind(a_latency, b_latency)\n",
    "\n",
    "print(f\"ðŸ”¬ Statistical Tests:\")\n",
    "print(f\"Score difference p-value: {score_test.pvalue:.6f}\")\n",
    "print(f\"Latency difference p-value: {latency_test.pvalue:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b14993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Box plots\n",
    "sns.boxplot(data=df, x='prompt_version', y='score', ax=ax1)\n",
    "ax1.set_title('Quality Score Distribution')\n",
    "\n",
    "sns.boxplot(data=df, x='prompt_version', y='latency_ms', ax=ax2)\n",
    "ax2.set_title('Latency Distribution')\n",
    "\n",
    "# Scatter plot\n",
    "for version in ['A', 'B']:\n",
    "    subset = df[df['prompt_version'] == version]\n",
    "    ax3.scatter(subset['latency_ms'], subset['score'], label=f'Prompt {version}', alpha=0.7)\n",
    "ax3.set_xlabel('Latency (ms)')\n",
    "ax3.set_ylabel('Score')\n",
    "ax3.set_title('Score vs Latency')\n",
    "ax3.legend()\n",
    "\n",
    "# Category performance\n",
    "category_perf = df.groupby(['category', 'prompt_version'])['score'].mean().unstack()\n",
    "category_perf.plot(kind='bar', ax=ax4)\n",
    "ax4.set_title('Performance by Category')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c68f5ee",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### Performance Results\n",
    "- Prompt A excels in latency (faster responses)\n",
    "- Prompt B shows higher quality scores for complex queries\n",
    "- Statistical significance confirms hypothesis\n",
    "\n",
    "### Recommendations\n",
    "- Use Prompt A for quick, factual queries\n",
    "- Use Prompt B for detailed explanations and creative tasks\n",
    "- Consider hybrid approach based on query classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### Performance Results\n",
    "- Prompt A excels in latency (faster responses)\n",
    "- Prompt B shows higher quality scores for complex queries\n",
    "- Statistical significance confirms hypothesis\n",
    "\n",
    "### Recommendations\n",
    "- Use Prompt A for quick, factual queries\n",
    "- Use Prompt B for detailed explanations and creative tasks\n",
    "- Consider hybrid approach based on query classification"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
